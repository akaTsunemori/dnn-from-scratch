<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Source of the Rust file `dnn_from_scratch/src/activation.rs`."><title>activation.rs - source</title><script>if(window.location.protocol!=="file:")document.head.insertAdjacentHTML("beforeend","SourceSerif4-Regular-6b053e98.ttf.woff2,FiraSans-Regular-0fe48ade.woff2,FiraSans-Medium-e1aa3f0a.woff2,SourceCodePro-Regular-8badfe75.ttf.woff2,SourceCodePro-Semibold-aa29a496.ttf.woff2".split(",").map(f=>`<link rel="preload" as="font" type="font/woff2" crossorigin href="../../static.files/${f}">`).join(""))</script><link rel="stylesheet" href="../../static.files/normalize-9960930a.css"><link rel="stylesheet" href="../../static.files/rustdoc-faa87aeb.css"><meta name="rustdoc-vars" data-root-path="../../" data-static-root-path="../../static.files/" data-current-crate="dnn_from_scratch" data-themes="" data-resource-suffix="" data-rustdoc-version="1.86.0-nightly (f7cc13af8 2025-01-25)" data-channel="nightly" data-search-js="search-ccb196c1.js" data-settings-js="settings-0f613d39.js" ><script src="../../static.files/storage-59e33391.js"></script><script defer src="../../static.files/src-script-56102188.js"></script><script defer src="../../src-files.js"></script><script defer src="../../static.files/main-5f194d8c.js"></script><noscript><link rel="stylesheet" href="../../static.files/noscript-893ab5e7.css"></noscript><link rel="alternate icon" type="image/png" href="../../static.files/favicon-32x32-6580c154.png"><link rel="icon" type="image/svg+xml" href="../../static.files/favicon-044be391.svg"></head><body class="rustdoc src"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="sidebar"><div class="src-sidebar-title"><h2>Files</h2></div></nav><div class="sidebar-resizer"></div><main><rustdoc-search></rustdoc-search><section id="main-content" class="content"><div class="main-heading"><h1><div class="sub-heading">dnn_from_scratch/</div>activation.rs</h1><rustdoc-toolbar></rustdoc-toolbar></div><div class="example-wrap"><div data-nosnippet><pre class="src-line-numbers">
<a href="#1" id="1">1</a>
<a href="#2" id="2">2</a>
<a href="#3" id="3">3</a>
<a href="#4" id="4">4</a>
<a href="#5" id="5">5</a>
<a href="#6" id="6">6</a>
<a href="#7" id="7">7</a>
<a href="#8" id="8">8</a>
<a href="#9" id="9">9</a>
<a href="#10" id="10">10</a>
<a href="#11" id="11">11</a>
<a href="#12" id="12">12</a>
<a href="#13" id="13">13</a>
<a href="#14" id="14">14</a>
<a href="#15" id="15">15</a>
<a href="#16" id="16">16</a>
<a href="#17" id="17">17</a>
<a href="#18" id="18">18</a>
<a href="#19" id="19">19</a>
<a href="#20" id="20">20</a>
<a href="#21" id="21">21</a>
<a href="#22" id="22">22</a>
<a href="#23" id="23">23</a>
<a href="#24" id="24">24</a>
<a href="#25" id="25">25</a>
<a href="#26" id="26">26</a>
<a href="#27" id="27">27</a>
<a href="#28" id="28">28</a>
<a href="#29" id="29">29</a>
<a href="#30" id="30">30</a>
<a href="#31" id="31">31</a>
<a href="#32" id="32">32</a>
<a href="#33" id="33">33</a>
<a href="#34" id="34">34</a>
<a href="#35" id="35">35</a>
<a href="#36" id="36">36</a>
<a href="#37" id="37">37</a>
<a href="#38" id="38">38</a>
<a href="#39" id="39">39</a>
<a href="#40" id="40">40</a>
<a href="#41" id="41">41</a>
<a href="#42" id="42">42</a>
<a href="#43" id="43">43</a>
<a href="#44" id="44">44</a>
<a href="#45" id="45">45</a>
<a href="#46" id="46">46</a>
<a href="#47" id="47">47</a>
<a href="#48" id="48">48</a>
<a href="#49" id="49">49</a>
<a href="#50" id="50">50</a>
<a href="#51" id="51">51</a>
<a href="#52" id="52">52</a>
<a href="#53" id="53">53</a>
<a href="#54" id="54">54</a>
<a href="#55" id="55">55</a>
<a href="#56" id="56">56</a>
<a href="#57" id="57">57</a>
<a href="#58" id="58">58</a>
<a href="#59" id="59">59</a>
<a href="#60" id="60">60</a>
<a href="#61" id="61">61</a>
<a href="#62" id="62">62</a>
<a href="#63" id="63">63</a>
<a href="#64" id="64">64</a>
<a href="#65" id="65">65</a>
<a href="#66" id="66">66</a>
<a href="#67" id="67">67</a>
<a href="#68" id="68">68</a>
<a href="#69" id="69">69</a>
<a href="#70" id="70">70</a>
<a href="#71" id="71">71</a>
<a href="#72" id="72">72</a>
<a href="#73" id="73">73</a>
<a href="#74" id="74">74</a>
<a href="#75" id="75">75</a>
<a href="#76" id="76">76</a>
<a href="#77" id="77">77</a>
<a href="#78" id="78">78</a>
<a href="#79" id="79">79</a>
<a href="#80" id="80">80</a>
<a href="#81" id="81">81</a>
<a href="#82" id="82">82</a>
<a href="#83" id="83">83</a>
<a href="#84" id="84">84</a>
<a href="#85" id="85">85</a>
<a href="#86" id="86">86</a>
<a href="#87" id="87">87</a>
<a href="#88" id="88">88</a>
<a href="#89" id="89">89</a>
<a href="#90" id="90">90</a>
<a href="#91" id="91">91</a>
<a href="#92" id="92">92</a>
<a href="#93" id="93">93</a>
<a href="#94" id="94">94</a>
<a href="#95" id="95">95</a>
<a href="#96" id="96">96</a>
<a href="#97" id="97">97</a>
<a href="#98" id="98">98</a>
<a href="#99" id="99">99</a>
<a href="#100" id="100">100</a>
<a href="#101" id="101">101</a>
<a href="#102" id="102">102</a>
<a href="#103" id="103">103</a>
<a href="#104" id="104">104</a>
<a href="#105" id="105">105</a>
<a href="#106" id="106">106</a>
<a href="#107" id="107">107</a></pre></div><pre class="rust"><code><span class="doccomment">//! # Activation Module
//!
//! This module provides a structure and functionality to handle activation functions commonly used
//! in neural networks. It supports the following activation types:
//!
//! - **ReLU (Rectified Linear Unit)**: Used to introduce non-linearity by outputting the input
//! directly if it is positive; otherwise, it outputs zero.
//! - **Softmax**: Converts a vector of real numbers into a probability distribution, typically
//! used in the output layer for multi-class classification problems.
//! - **None**: A pass-through activation that does not modify the input or output.

</span><span class="kw">use </span>nd::{Array2, Axis};

<span class="doccomment">/// Enum representing supported activation types.
</span><span class="kw">enum </span>ActivationType {
    <span class="doccomment">/// Softmax activation function.
    </span>Softmax,
    <span class="doccomment">/// ReLU activation function.
    </span>ReLU,
    <span class="doccomment">/// No activation function (pass-through).
    </span><span class="prelude-val">None</span>,
}

<span class="doccomment">/// The `Activation` structure manages activation functions and their forward
/// and backward passes.
</span><span class="kw">pub struct </span>Activation {
    activation_type: ActivationType,
}

<span class="kw">impl </span>Activation {
    <span class="doccomment">/// Creates a new `Activation` instance based on the specified type.
    ///
    /// # Arguments
    /// - `activation_type`: A string specifying the type of activation.
    ///   Supported values are `"relu"`, `"softmax"`, and `"none"`.
    ///
    /// # Panics
    /// This function will panic if an invalid activation type is provided.
    </span><span class="kw">pub fn </span>new(activation_type: <span class="kw-2">&amp;</span>str) -&gt; Activation {
        <span class="kw">let </span>activation_type = <span class="kw">match </span>activation_type {
            <span class="string">"relu" </span>=&gt; ActivationType::ReLU,
            <span class="string">"softmax" </span>=&gt; ActivationType::Softmax,
            <span class="string">"none" </span>=&gt; ActivationType::None,
            <span class="kw">_ </span>=&gt; <span class="macro">panic!</span>(<span class="string">"Invalid activation type."</span>),
        };
        Activation { activation_type }
    }

    <span class="doccomment">/// Performs the forward pass of the activation function on the input array.
    ///
    /// # Arguments
    /// - `z`: A 2D array (`Array2&lt;f64&gt;`) containing the input data.
    ///
    /// # Returns
    /// A 2D array with the activation function applied element-wise (ReLU) or row-wise (Softmax).
    </span><span class="kw">pub fn </span>forward(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="kw-2">mut </span>z: Array2&lt;f64&gt;) -&gt; Array2&lt;f64&gt; {
        <span class="kw">match </span><span class="self">self</span>.activation_type {
            ActivationType::ReLU =&gt; z.mapv(|v| v.max(<span class="number">0.</span>)),
            ActivationType::Softmax =&gt; {
                <span class="kw">for </span><span class="kw-2">mut </span>row <span class="kw">in </span>z.rows_mut() {
                    <span class="kw">let </span>z_max = row.iter().fold(f64::NEG_INFINITY, |max, <span class="kw-2">&amp;</span>v| max.max(v));
                    row.iter_mut().for_each(|v| <span class="kw-2">*</span>v -= z_max);
                }
                <span class="kw">let </span><span class="kw-2">mut </span>exp_values = z.mapv(|v| v.exp());
                <span class="kw">for </span><span class="kw-2">mut </span>row <span class="kw">in </span>exp_values.rows_mut() {
                    <span class="kw">let </span>sum: f64 = row.iter().sum();
                    row.iter_mut().for_each(|v| <span class="kw-2">*</span>v /= sum);
                }
                exp_values
            }
            ActivationType::None =&gt; z.to_owned(),
        }
    }

    <span class="doccomment">/// Performs the backward pass (gradient calculation) of the activation function.
    ///
    /// # Arguments
    /// - `d_values`: A 2D array containing gradients propagated from the subsequent layer.
    /// - `z`: A 2D array containing the output of the forward pass (or the input to the activation
    /// function).
    ///
    /// # Returns
    /// A 2D array with gradients adjusted based on the activation function.
    </span><span class="kw">pub fn </span>backward(<span class="kw-2">&amp;</span><span class="self">self</span>, d_values: <span class="kw-2">&amp;</span>Array2&lt;f64&gt;, z: <span class="kw-2">&amp;</span>Array2&lt;f64&gt;) -&gt; Array2&lt;f64&gt; {
        <span class="kw">let </span><span class="kw-2">mut </span>d_values = d_values.clone();
        <span class="kw">match </span><span class="self">self</span>.activation_type {
            ActivationType::ReLU =&gt; d_values <span class="kw-2">*</span>= <span class="kw-2">&amp;</span>z.mapv(|x| <span class="kw">if </span>x &gt; <span class="number">0. </span>{ <span class="number">1. </span>} <span class="kw">else </span>{ <span class="number">0. </span>}),
            ActivationType::Softmax =&gt; {
                <span class="kw">for </span>i <span class="kw">in </span><span class="number">0</span>..d_values.nrows() {
                    <span class="kw">let </span>gradient = d_values.row(i).to_owned();
                    <span class="kw">let </span>diagonal = Array2::from_diag(<span class="kw-2">&amp;</span>gradient);
                    <span class="kw">let </span>outer_product = gradient
                        .clone()
                        .insert_axis(Axis(<span class="number">1</span>))
                        .dot(<span class="kw-2">&amp;</span>gradient.clone().insert_axis(Axis(<span class="number">0</span>)));
                    <span class="kw">let </span>jacobian_matrix = diagonal - outer_product;
                    <span class="kw">let </span>transformed_gradient =
                        jacobian_matrix.dot(<span class="kw-2">&amp;</span>z.row(i).to_owned().insert_axis(Axis(<span class="number">1</span>)));
                    <span class="kw">let </span>result = transformed_gradient.index_axis(Axis(<span class="number">1</span>), <span class="number">0</span>);
                    d_values.row_mut(i).assign(<span class="kw-2">&amp;</span>result);
                }
            }
            ActivationType::None =&gt; {}
        }
        d_values
    }
}
</code></pre></div></section></main></body></html>